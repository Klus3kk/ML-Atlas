# Long Short-Term Memory Networks (LSTM)

## Overview

Long Short-Term Memory (LSTM) networks are a type of RNN designed to remember long-term dependencies. They address the vanishing gradient problem in standard RNNs, making them effective for learning sequences over long time intervals.

## Where is it Used?

- **Speech Recognition:** Understanding sequences of spoken words.
- **Text Generation:** Generating text in a sequence.
- **Machine Translation:** Translating languages in a sequence.

## How Does it Work?

LSTMs have a more complex structure than standard RNNs, with gates that regulate the flow of information. These gates control what information is kept, updated, or discarded, enabling the network to maintain long-term dependencies.
