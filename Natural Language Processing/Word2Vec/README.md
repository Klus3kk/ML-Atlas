# Word2Vec

## Overview

Word2Vec is a popular word embedding technique that represents words as dense vectors in a continuous vector space. It captures semantic meanings and relationships between words based on their context in a corpus.

## Where is it Used?

- **Semantic Analysis:** Understands the meaning of words and their relationships.
- **Text Classification:** Improves classification performance by representing words as vectors.
- **Translation:** Enhances machine translation systems.

## How Does it Work?

Word2Vec uses two main models:
- **Continuous Bag of Words (CBOW):** Predicts the target word based on the context words.
- **Skip-gram:** Predicts context words based on the target word.
